PET Project

Я рад представить вам мой проект, над которым я работал на протяжении последнего года. Он представляет собой мою личную интерпретацию облачного гейминга (cloud gaming) и был вдохновлён такими проектами, как Google Stadia и Microsoft XCloud. Основная идея проекта заключается в том, чтобы предоставить пользователям возможность запускать AAA-игры на тонких клиентах (таких как телефоны, планшеты и ПК без видеокарт, необходимых для сложных графических вычислений) посредством облачных технологий.

### Принцип работы платформы

Frontend: представляет список игр, установленных на удалённом сервере. После выбора игры начинается захват подключённого геймпада, и его состояние отправляется на сервер. На экране выводится поток (stream) запущенной игры.

Backend: запускает игру в контейнере, создаёт виртуальное устройство ввода и виртуальный дисплей, которые передаются в контейнер в качестве ресурсов. В контейнере производится захват видео и аудио, после чего данные передаются на клиент.

Принцип работы на схеме: ***

*

### Текущее состояние проекта

- Проект доведён до рабочего, технически исправного состояния.
- Заложен фундамент для возможности масштабирования.
- Проект заморожен.

*

### От автора

Этот проект стал для меня практическим полигоном для освоения и тестирования новых технологий и языков программирования. Вместо написания очередного To-Do листа я выбрал более амбициозную идею — создание платформы облачного гейминга.

Проект был написан с нуля и собран "from scratch". Он доведён до технически исправного состояния, но скорее всего останется на этом уровне развития. У меня нет дальнейшей мотивации развивать этот проект, прежде всего потому, что подобные решения уже существуют и обладают значительно более богатой функциональностью. Некоторые из них уже прекратили своё существование и забыты.

Моя цель была доказать самому себе, что я смогу это сделать.

Заметка для тех, кто захочет собрать и запустить проект у себя: ***

Характеристики серверного оборудования:
Мой личный домашний ПК под управлением Arch Linux.

Железо:
***

Благодарности за помощь с настройкой системы и оптимизацией: ***

Текущие версии используемых пакетов: ***

Для написания этой документации я разворачивал проект на новой системе. Вот технологии и утилиты, которые необходимы на сервере для запуска проекта:
***

### Проблемы, с которыми я столкнулся при запуске на свежей системе, и пути их решения

1. Виртуальные дисплеи для игрового процесса. Возможно, существует способ внедрения DLL hook непосредственно в саму игру для того, чтобы не направлять вывод на экран, а сразу передавать кадры в трансляцию. Либо это можно сделать на этапе рендеринга (я не эксперт в этих вопросах). Однако, такой подход потребовал бы глубоких знаний низкоуровневой разработки, либо, как в случае с DLL, прямого разрешения от компаний на внесение изменений в их код. Поэтому был выбран подход с рендерингом и захватом изображения с виртуальных экранов.

   Из доступных решений есть Xvfb, xhyperyl, а также, возможно, существуют решения на базе Wayland, хотя они могут быть сырыми. Первые два не поддерживают DRI3 (либо мне не удалось их запустить), который требуется для работы современных технологий, таких как Vulkan и DirectX. Однако, они до сих пор могут использоваться для игр на основе OpenGL или старых версий DirectX.

   Для современных игр необходим нативный X11 сервер. Лучшее решение, которое я нашёл в интернете — это покупка HDMI-заглушек, эмулирующих подключенный дисплей через встроенный в них файл EDID. Но я нашёл другое решение — использование Zaphor Head Configuration для Xorg. Этот метод был популярен во времена multi seat  систем. С помощью такого конфига и утилиты xrandr мне удалось включить все доступные экраны (в пределах количества видеовыходов на видеокарте).
   При таком запуске каждый экран получает уникальные crtcID и planeID, которые можно удобно использовать для захвата области виртуального экрана с помощью ffmpeg и опции kms.

   Проблемы, которые возникли: видеокарта определялась в системе как /dev/dri/card1 вместо /dev/dri/card0, что мешало запуску экранов. Ни одна из версий драйверов не могла решить эту проблему, включая указание модулей amdgpu и radeon. Вот тикет и решение, которое мне удалось найти: https://bbs.archlinux.org/viewtopic.php?id=288578 (решение:add kernel parameter "initcall_blacklist=simpledrm_platform_driver_init"). Проблема касается новых устройств от AMD и свежих драйверов для них.

2. ffmpeg и запись экрана. Как упоминалось выше, захват с виртуальных экранов осуществляется с использованием kms, что по умолчанию включено и не вызывает проблем. Однако на свежей ОС я столкнулся с ошибкой "can not allocate memory". Точно не уверен, с чем это было связано — с отсутствием файла подкачки или чем-то ещё. Проблема решилась установкой более свежей Git-версии ffmpeg. Вот пример флагов, которые используются для захвата видео: ***.

   Однако остаётся один ворнинг, от которого я так и не смог избавиться: ffmpeg при кодировании постоянно жалуется на: *. Нашёл подобный тикет: *, но не смог решить проблему, хотя видимых последствий это не вызывает. Так как передача медиа осуществляется с использованием WebRTC, кодек H264 не является лучшим решением; гораздо лучше использовать VP8. В ходе тестов при использовании VP8 наблюдалась значительно меньшая потеря пакетов по сравнению с H264. Почему так происходит, для меня остаётся загадкой. Я не смог запустить VAAPI**-акселерацию для **VP8, поэтому тесты проводились с перекодированием в VP8 из H264 на процессоре.

   Важная пометка: WebRTC считывает RTP-поток, который передаётся на loopback (127.0.0.1). Согласно документации, размер пакетов не должен превышать 1200 байт, но я запускал при 1400 байт на пакет, что приводит к уменьшению общего количества пакетов.

3. Контейнеризация процессов. После мучительного выбора предпочтение было отдано Docker, но я рекомендую присмотреться к Conty от *. Docker был выбран из-за удобного API, с помощью библиотеки *() можно в рантайме создавать и запускать контейнеры. Для каждого процесса был создан свой контейнер: **ffmpeg для захвата видео, ffmpeg для захвата аудио, контейнер для запуска игры и контейнер, в котором запущен аудиосервер для обработки звука. Эти контейнеры должны запускаться группой.

   Для упрощения можно было бы запускать всё в одном контейнере, но это является плохой практикой, затрудняет тестирование и вызывает одну проблему: при трансляции захваченного потока после его доставки на клиент трансляция может замирать на неопределённое время, хотя пакеты продолжают отправляться и лог WebRTC не сигнализирует об ошибках. Эту проблему я решил разделением процессов по разным контейнерам.

   Была попытка запускать Docker Compose через скрипт, просто передавая файл с переменными (Docker API не поддерживает **Docker Compose**), но это вызвало другие ошибки, связанные с доступом к общим директориям и файлам, которые используют контейнеры. Поэтому синхронный запуск управляется серверной программой, хотя она пока и не имеет лаконичного завершения работы группы контейнеров в случае ошибки.

Что ещё хотелось бы реализовать :
